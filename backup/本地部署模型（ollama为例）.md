# 本地部署模型（ollama为例）
>**前言** ：这个方法是部署模型最简单的方法，但不一定是最好的 **（取决于你的电脑系统 or 配置）** 大家酌情考虑
### 为什么需要本地部署？
>相信你之前也遇到过类似的情况：
1. 在外面**没有网络**，依靠***百度战神***查资料是不可能了，十分不便
2. 想玩一些开源大模型【如：**ollama GML gemma** 等】，访问这类网站在国内是要被墙的，**挂VPN又《可刑可铐》**，或者是找一个***镜像站***既不安全，又不稳定，令人苦恼
3. 没有浏览器（虽然大多数都有）
>如果你是大陆用户，或者是恰恰有遇到以上的这些问题，那这个方法就很适合你！
###### 不
###### 多
###### 说
#### 上操作

---

#### 访问官网/下载环境
链接【以***ollama***为例】

**[点击跳转](https://ollama.com/)**

>**点击 [Download](https://ollama.com/)**
选择适合你的系统下载版本（**Windows & MacOs & Linux**）
这里以 **Windows** 为例：
运行下载好的安装程序 ***Ollama Setup.exe*** 这里**点击 `Install` 即可**

---

#### 搜索/部署模型
>***Ollama Setup.exe*** 安装好以后就可以**搜索并部署模型**了(～￣▽￣)～
方法也是 ***`肥肠`*** 的简单啊~
##### 子标题：搜索
- **回到 ollama 官网** **[点击跳转](https://ollama.com/)** ，并点击上方的搜索栏，在搜索栏中***搜索你想要的模型编号即可***【如：ollama GML gemma 等】
- 点击你想要的模型后 **（以 llama 3.1 模型为例）** **[点击示例](https://ollama.com/library/llama3.1)** 在此界面中你可以**选择模型的型号 *【如：8b 70b 405b】*** 左侧即是安装命令了
###### Up 自己整理理了一些常用的开源模型，如下：
-  **[Ollama 3.1](https://ollama.com/library/llama3.1:8b)** **(8b 70b 405b 同理)**

-  **[gemma 2](https://ollama.com/library/gemma2)** **(2b 9b 27b)**

-  **[phi 3.5](https://ollama.com/library/phi3.5)** **(3.8b模型)**

##### 子标题：部署
1. 打开 **`PowerShell`** ，为了万无一失建议先输入**ollama**，查看一下当前运行情况如果**有输出**则没问题
2. 输入模型对应的安装命令【如**ollama**的模型部署命令就是 **{ollama run llama3.1:8b}**】即可

---

#### 模型要求
>Windows：**3060以上**显卡+**8G以上**显存+**16G内存**，硬盘空间至少**20G**
Mac：**M1或M2芯片** 16G内存，**20G以上硬盘空间**